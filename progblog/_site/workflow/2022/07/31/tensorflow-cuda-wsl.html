<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="http://code.jquery.com/jquery-3.2.1.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/mousetrap/1.4.6/mousetrap.min.js"></script>
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/css/searchoverlay.css" rel="stylesheet"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Installing Tensorflow on WSL with CUDA support | ProgBlog</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Installing Tensorflow on WSL with CUDA support" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Getting tensorflow to work properly on WSL with GPU support can be a little difficult. One needs to use the right dependencies for the a particular version of tensorflow.1 The dependencies are listed here for the latest version. For older versions, it is listed here instead. My hardware configuration looks like this: CPU: AMD Ryzen 5800H (8, 16) GPU: NVIDIA GeForce RTX 3050Ti (Laptop) RAM: 16 GB (8 GB allowed on WSL2) Software: Ubuntu 22.04 LTS on WSL2 NVIDIA Graphics Driver (Game-ready) 516.59 CUDA toolkit 11.2 (11.7 supported) cuDNN 8.1.1 Python 3.10.4 pip 22.2.1 tensorflow 2.9.1 Most of this is inspired from this guide. However, parts of it didn’t work for me and I had to tweak. We start with installing the cuda toolkit. In our case, we have to choose version 11.2 (the latest is 11.7), so we must dive into the NVIDIA archives and choose the right version. Then we select Linux -&gt; x86_64 -&gt; WSL-Ubuntu -&gt; 2.0 -&gt; deb (local). I use the local deb installer, however, this is upto the user. At this point, if any other versions of cuda are installed, we can uninstall them with sudo apt --purge remove cuda sudo apt autoremove This will work only if cuda was installed with apt. Otherwise, one can follow the instructions in this stackoverflow thread. To remove references to the cuda repo (since the reference might be version-specific), we have two options: Edit /etc/apt/sources.list if it contains references to the NVIDIA cuda repo. Delete the key in /etc/apt/sources.list.d that refers to the same. We also delete the old apt key: sudo apt-key del 7fa2af80 Now we can proceed with the installation.2 (The following is from the installation instructions for v11.2.0. The full guide for installing CUDA on WSL is here): wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.2.0/local_installers/cuda-repo-wsl-ubuntu-11-2-local_11.2.0-1_amd64.deb sudo dpkg -i cuda-repo-wsl-ubuntu-11-2-local_11.2.0-1_amd64.deb sudo apt-key add /var/cuda-repo-wsl-ubuntu-11-2-local/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda Get a coffee - this might take time! Meanwhile, we can edit our rc file (.bashrc or .zshrc or whatever is relevant) and add the following lines: export PATH=&quot;/usr/local/cuda/bin:$PATH&quot; export LD_LIBRARY_PATH=&quot;/usr/local/cuda/lib64:$LD_LIBRARY_PATH&quot; Finally, we source the rc file to load the changes. Now we install cuDNN. Installation instructions are here. We just get the library files of cuDNN and copy them to the appropriate location. We do not use an installer. First, we install zlib1g, a compression library required by cuDNN (in my case, it was already installed): sudo apt install zlib1g Now we have to sign up for the NVIDIA Developer Program first. Once we’re done and are signed in, we can proceed to downloading cuDNN. We go the cuDNN archive and choose the appropriate version (v8.1.1 - the most recent version is listed here instead). We choose the “cuDNN Library for Linux (x86_64)” option, which downloads a tar file. We move this tar file to our WSL distro (for quicker file ops) and extract this using tar xvf &lt;filename&gt; where &lt;filename&gt; should start with cudnn-x.y- where x.y is the cuda version. From the folder in which files were extracted, we copy these files like so: sudo cp &lt;extracted-folder-name&gt;/include/cudnn*.h /usr/local/cuda/include sudo cp -P &lt;extracted-folder-name&gt;/lib/libcudnn* /usr/local/cuda/lib64 and change the permissions of these files to allow all users to read them: sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* Finally, we install tensorflow: pip install tensorflow This will also, likely, take time. After that, we’re done with the installation! Now we can run this in a python shell to verify our import tensorflow as tf print(tf.config.list_physical_devices(&#39;GPU&#39;)) If this shows a non-empty list, tensorflow recognizes the gpu and the necessary libraries.3 Typically, this output will look something like [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] This naturally calls for the use of a virtual environment specifically for development involving tensorflow but we will be skipping this adventure here. This is traditionally done with conda - as most of the guides recommend. However, issues have been reported on WSL using this method. We can use a typical virtual environment (virtualenv) instead. &#8617; The display driver must be installed on Windows. No separate installation of a graphics drivers is required on WSL. Standard cuda toolkits for Linux include the driver - so we cannot choose those versions. &#8617; If a warning is given by tensorflow about Your kernel may have been built without NUMA support.: this is probably nothing to worry about, as discussed in this thread. &#8617;" />
<meta property="og:description" content="Getting tensorflow to work properly on WSL with GPU support can be a little difficult. One needs to use the right dependencies for the a particular version of tensorflow.1 The dependencies are listed here for the latest version. For older versions, it is listed here instead. My hardware configuration looks like this: CPU: AMD Ryzen 5800H (8, 16) GPU: NVIDIA GeForce RTX 3050Ti (Laptop) RAM: 16 GB (8 GB allowed on WSL2) Software: Ubuntu 22.04 LTS on WSL2 NVIDIA Graphics Driver (Game-ready) 516.59 CUDA toolkit 11.2 (11.7 supported) cuDNN 8.1.1 Python 3.10.4 pip 22.2.1 tensorflow 2.9.1 Most of this is inspired from this guide. However, parts of it didn’t work for me and I had to tweak. We start with installing the cuda toolkit. In our case, we have to choose version 11.2 (the latest is 11.7), so we must dive into the NVIDIA archives and choose the right version. Then we select Linux -&gt; x86_64 -&gt; WSL-Ubuntu -&gt; 2.0 -&gt; deb (local). I use the local deb installer, however, this is upto the user. At this point, if any other versions of cuda are installed, we can uninstall them with sudo apt --purge remove cuda sudo apt autoremove This will work only if cuda was installed with apt. Otherwise, one can follow the instructions in this stackoverflow thread. To remove references to the cuda repo (since the reference might be version-specific), we have two options: Edit /etc/apt/sources.list if it contains references to the NVIDIA cuda repo. Delete the key in /etc/apt/sources.list.d that refers to the same. We also delete the old apt key: sudo apt-key del 7fa2af80 Now we can proceed with the installation.2 (The following is from the installation instructions for v11.2.0. The full guide for installing CUDA on WSL is here): wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.2.0/local_installers/cuda-repo-wsl-ubuntu-11-2-local_11.2.0-1_amd64.deb sudo dpkg -i cuda-repo-wsl-ubuntu-11-2-local_11.2.0-1_amd64.deb sudo apt-key add /var/cuda-repo-wsl-ubuntu-11-2-local/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda Get a coffee - this might take time! Meanwhile, we can edit our rc file (.bashrc or .zshrc or whatever is relevant) and add the following lines: export PATH=&quot;/usr/local/cuda/bin:$PATH&quot; export LD_LIBRARY_PATH=&quot;/usr/local/cuda/lib64:$LD_LIBRARY_PATH&quot; Finally, we source the rc file to load the changes. Now we install cuDNN. Installation instructions are here. We just get the library files of cuDNN and copy them to the appropriate location. We do not use an installer. First, we install zlib1g, a compression library required by cuDNN (in my case, it was already installed): sudo apt install zlib1g Now we have to sign up for the NVIDIA Developer Program first. Once we’re done and are signed in, we can proceed to downloading cuDNN. We go the cuDNN archive and choose the appropriate version (v8.1.1 - the most recent version is listed here instead). We choose the “cuDNN Library for Linux (x86_64)” option, which downloads a tar file. We move this tar file to our WSL distro (for quicker file ops) and extract this using tar xvf &lt;filename&gt; where &lt;filename&gt; should start with cudnn-x.y- where x.y is the cuda version. From the folder in which files were extracted, we copy these files like so: sudo cp &lt;extracted-folder-name&gt;/include/cudnn*.h /usr/local/cuda/include sudo cp -P &lt;extracted-folder-name&gt;/lib/libcudnn* /usr/local/cuda/lib64 and change the permissions of these files to allow all users to read them: sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* Finally, we install tensorflow: pip install tensorflow This will also, likely, take time. After that, we’re done with the installation! Now we can run this in a python shell to verify our import tensorflow as tf print(tf.config.list_physical_devices(&#39;GPU&#39;)) If this shows a non-empty list, tensorflow recognizes the gpu and the necessary libraries.3 Typically, this output will look something like [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] This naturally calls for the use of a virtual environment specifically for development involving tensorflow but we will be skipping this adventure here. This is traditionally done with conda - as most of the guides recommend. However, issues have been reported on WSL using this method. We can use a typical virtual environment (virtualenv) instead. &#8617; The display driver must be installed on Windows. No separate installation of a graphics drivers is required on WSL. Standard cuda toolkits for Linux include the driver - so we cannot choose those versions. &#8617; If a warning is given by tensorflow about Your kernel may have been built without NUMA support.: this is probably nothing to worry about, as discussed in this thread. &#8617;" />
<link rel="canonical" href="http://localhost:4000/workflow/2022/07/31/tensorflow-cuda-wsl.html" />
<meta property="og:url" content="http://localhost:4000/workflow/2022/07/31/tensorflow-cuda-wsl.html" />
<meta property="og:site_name" content="ProgBlog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-31T12:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Installing Tensorflow on WSL with CUDA support" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/workflow/2022/07/31/tensorflow-cuda-wsl.html","headline":"Installing Tensorflow on WSL with CUDA support","dateModified":"2022-07-31T12:00:00+05:30","datePublished":"2022-07-31T12:00:00+05:30","description":"Getting tensorflow to work properly on WSL with GPU support can be a little difficult. One needs to use the right dependencies for the a particular version of tensorflow.1 The dependencies are listed here for the latest version. For older versions, it is listed here instead. My hardware configuration looks like this: CPU: AMD Ryzen 5800H (8, 16) GPU: NVIDIA GeForce RTX 3050Ti (Laptop) RAM: 16 GB (8 GB allowed on WSL2) Software: Ubuntu 22.04 LTS on WSL2 NVIDIA Graphics Driver (Game-ready) 516.59 CUDA toolkit 11.2 (11.7 supported) cuDNN 8.1.1 Python 3.10.4 pip 22.2.1 tensorflow 2.9.1 Most of this is inspired from this guide. However, parts of it didn’t work for me and I had to tweak. We start with installing the cuda toolkit. In our case, we have to choose version 11.2 (the latest is 11.7), so we must dive into the NVIDIA archives and choose the right version. Then we select Linux -&gt; x86_64 -&gt; WSL-Ubuntu -&gt; 2.0 -&gt; deb (local). I use the local deb installer, however, this is upto the user. At this point, if any other versions of cuda are installed, we can uninstall them with sudo apt --purge remove cuda sudo apt autoremove This will work only if cuda was installed with apt. Otherwise, one can follow the instructions in this stackoverflow thread. To remove references to the cuda repo (since the reference might be version-specific), we have two options: Edit /etc/apt/sources.list if it contains references to the NVIDIA cuda repo. Delete the key in /etc/apt/sources.list.d that refers to the same. We also delete the old apt key: sudo apt-key del 7fa2af80 Now we can proceed with the installation.2 (The following is from the installation instructions for v11.2.0. The full guide for installing CUDA on WSL is here): wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.2.0/local_installers/cuda-repo-wsl-ubuntu-11-2-local_11.2.0-1_amd64.deb sudo dpkg -i cuda-repo-wsl-ubuntu-11-2-local_11.2.0-1_amd64.deb sudo apt-key add /var/cuda-repo-wsl-ubuntu-11-2-local/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda Get a coffee - this might take time! Meanwhile, we can edit our rc file (.bashrc or .zshrc or whatever is relevant) and add the following lines: export PATH=&quot;/usr/local/cuda/bin:$PATH&quot; export LD_LIBRARY_PATH=&quot;/usr/local/cuda/lib64:$LD_LIBRARY_PATH&quot; Finally, we source the rc file to load the changes. Now we install cuDNN. Installation instructions are here. We just get the library files of cuDNN and copy them to the appropriate location. We do not use an installer. First, we install zlib1g, a compression library required by cuDNN (in my case, it was already installed): sudo apt install zlib1g Now we have to sign up for the NVIDIA Developer Program first. Once we’re done and are signed in, we can proceed to downloading cuDNN. We go the cuDNN archive and choose the appropriate version (v8.1.1 - the most recent version is listed here instead). We choose the “cuDNN Library for Linux (x86_64)” option, which downloads a tar file. We move this tar file to our WSL distro (for quicker file ops) and extract this using tar xvf &lt;filename&gt; where &lt;filename&gt; should start with cudnn-x.y- where x.y is the cuda version. From the folder in which files were extracted, we copy these files like so: sudo cp &lt;extracted-folder-name&gt;/include/cudnn*.h /usr/local/cuda/include sudo cp -P &lt;extracted-folder-name&gt;/lib/libcudnn* /usr/local/cuda/lib64 and change the permissions of these files to allow all users to read them: sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* Finally, we install tensorflow: pip install tensorflow This will also, likely, take time. After that, we’re done with the installation! Now we can run this in a python shell to verify our import tensorflow as tf print(tf.config.list_physical_devices(&#39;GPU&#39;)) If this shows a non-empty list, tensorflow recognizes the gpu and the necessary libraries.3 Typically, this output will look something like [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] This naturally calls for the use of a virtual environment specifically for development involving tensorflow but we will be skipping this adventure here. This is traditionally done with conda - as most of the guides recommend. However, issues have been reported on WSL using this method. We can use a typical virtual environment (virtualenv) instead. &#8617; The display driver must be installed on Windows. No separate installation of a graphics drivers is required on WSL. Standard cuda toolkits for Linux include the driver - so we cannot choose those versions. &#8617; If a warning is given by tensorflow about Your kernel may have been built without NUMA support.: this is probably nothing to worry about, as discussed in this thread. &#8617;","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/workflow/2022/07/31/tensorflow-cuda-wsl.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="ProgBlog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">ProgBlog</a>

    <script>
      $(document).ready(function() {
      $('#close-btn').click(function() {
        $('#search-overlay').fadeOut();
        $('#search-btn').show();
      });
      $('#search-btn').click(function() {
        $(this).hide();
        $('#search-overlay').fadeIn();
      });
    });
    </script>
    
    <i id="search-btn" class="fa fa-search fa-2x"></i>
    <div id="search-overlay" class="block">
      <div class="centered">
        <!-- <div id='search-box'>
          <i id="close-btn" class="fa fa-times fa-2x"></i>
          <form action='/search' id='search-form' method='get' target='_top'>
            <input id='search-text' name='q' placeholder='Search' type='text' />
            <button id='search-button' type='submit'>                     
              <i id="search-btn" class="fa fa-search fa-2x"></i>
            </button>
          </form>
        </div> -->

        <!-- Html Elements for Search -->
        <div id="search-box">
          <i id="close-btn" class="fa fa-times fa-2x"></i>
          <input type="text" id="search-input" class="mousetrap" placeholder="Start searching...">
          <ul id="results-container"></ul>
        </div>
      </div>
    </div>
    <!-- Script pointing to search-script.js -->
    <script src="/js/search-script.js" type="text/javascript"></script>

    <!-- Configuration -->
    <script>
    var srch = SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('results-container'),
      json: '/search.json',
      searchResultTemplate: `
      <li>
        <div style="display:inline"><a href="{url}" title="{desc}">{title}</a></div>
        <div style="width:20%; float:right; border-left:2px solid #777"><span style="padding-left: 5px; opacity:0.75">{category}</span></div>
      </li>
      `
    })
    </script>

    <!-- Custom keybind to search -->
    <script>
      function showSearch() {
        if($('#close-btn').is(":hidden")) {
          $('#search-btn').click();
          $('#search-input').focus();
        }
      }
      function hideSearch() {
        if($('#search-btn').is(":hidden")) {
          $('#close-btn').click();
        }
      }
      Mousetrap.bind(['command+shift+f', 'ctrl+shift+f'], showSearch);
      Mousetrap.bind(['escape'], hideSearch);
    </script>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Installing Tensorflow on WSL with CUDA support</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-07-31T12:00:00+05:30" itemprop="datePublished">Jul 31, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    
<p>Getting <strong><code class="language-plaintext highlighter-rouge">tensorflow</code></strong> to work properly on WSL with GPU support can be a little difficult. One needs to use the right dependencies for the a particular version of tensorflow.<sup id="fnref:venv" role="doc-noteref"><a href="#fn:venv" class="footnote" rel="footnote">1</a></sup> The dependencies are listed <a href="https://www.tensorflow.org/install/pip">here</a> for the latest version. For older versions, it is listed <a href="https://www.tensorflow.org/install/source#tested_build_configurations">here instead</a>.</p>

<p>My hardware configuration looks like this:</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CPU: AMD Ryzen 5800H (8, 16)
GPU: NVIDIA GeForce RTX 3050Ti (Laptop)
RAM: 16 GB (8 GB allowed on WSL2)
</code></pre></div></div>
<p>Software:</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Ubuntu 22.04 LTS on WSL2
NVIDIA Graphics Driver (Game-ready) 516.59
CUDA toolkit 11.2 (11.7 supported)
cuDNN 8.1.1
Python 3.10.4
pip 22.2.1
tensorflow 2.9.1
</code></pre></div></div>
<p>Most of this is inspired from <a href="https://medium.com/@xizengmao/install-tensorflow-with-gpu-acceleration-simultaneously-for-windows-and-wsl-linux-2-10da088d5e4f">this guide</a>. However, parts of it didn’t work for me and I had to tweak.</p>

<p>We start with installing the cuda toolkit. In our case, we have to choose version 11.2 (the latest is 11.7), so we must dive into the NVIDIA <a href="https://developer.nvidia.com/cuda-toolkit-archive">archives</a> and choose the right version. Then we select <strong>Linux -&gt; x86_64 -&gt; WSL-Ubuntu -&gt; 2.0 -&gt; deb (local)</strong>. I use the local <strong>deb</strong> installer, however, this is upto the user.</p>

<p>At this point, if any other versions of cuda are installed, we can uninstall them with</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nt">--purge</span> remove cuda
<span class="nb">sudo </span>apt autoremove
</code></pre></div></div>
<p>This will work only if cuda was installed with apt. Otherwise, one can follow the instructions in this <a href="https://stackoverflow.com/questions/56431461/how-to-remove-cuda-completely-from-ubuntu">stackoverflow thread</a>. To remove references to the cuda repo (since the reference might be version-specific), we have two options:</p>
<ul>
  <li>Edit <em>/etc/apt/sources.list</em> if it contains references to the NVIDIA cuda repo.</li>
  <li>Delete the key in <em>/etc/apt/sources.list.d</em> that refers to the same.
We also delete the old apt key:
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-key del 7fa2af80
</code></pre></div>    </div>
  </li>
</ul>

<p>Now we can proceed with the installation.<sup id="fnref:drivernote" role="doc-noteref"><a href="#fn:drivernote" class="footnote" rel="footnote">2</a></sup> (The following is from the installation instructions for v11.2.0. The full guide for installing CUDA on WSL is <a href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html">here</a>):</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
<span class="nb">sudo mv </span>cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/11.2.0/local_installers/cuda-repo-wsl-ubuntu-11-2-local_11.2.0-1_amd64.deb
<span class="nb">sudo </span>dpkg <span class="nt">-i</span> cuda-repo-wsl-ubuntu-11-2-local_11.2.0-1_amd64.deb
<span class="nb">sudo </span>apt-key add /var/cuda-repo-wsl-ubuntu-11-2-local/7fa2af80.pub
<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nt">-y</span> <span class="nb">install </span>cuda
</code></pre></div></div>
<p>Get a coffee - this might take time!</p>

<p>Meanwhile, we can edit our rc file (.bashrc or .zshrc or whatever is relevant) and add the following lines:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">"/usr/local/cuda/bin:</span><span class="nv">$PATH</span><span class="s2">"</span>
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">"/usr/local/cuda/lib64:</span><span class="nv">$LD_LIBRARY_PATH</span><span class="s2">"</span>
</code></pre></div></div>
<p>Finally, we <em>source the rc file</em> to load the changes.</p>

<p>Now we install cuDNN. Installation instructions are <a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html">here</a>. We just get the library files of cuDNN and copy them to the appropriate location. We do not use an installer.
First, we install <code class="language-plaintext highlighter-rouge">zlib1g</code>, a compression library required by cuDNN (in my case, it was already installed):</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>zlib1g
</code></pre></div></div>
<p>Now we have to sign up for the NVIDIA Developer Program first. Once we’re done and are signed in, we can proceed to downloading cuDNN. We go the <a href="https://developer.nvidia.com/rdp/cudnn-archive">cuDNN archive</a> and choose the appropriate version (v8.1.1 - the most recent version is listed <a href="https://developer.nvidia.com/rdp/cudnn-download">here instead</a>). We choose the “cuDNN Library for Linux (x86_64)” option, which downloads a tar file. We move this tar file to our WSL distro (for quicker file ops) and extract this using</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">tar </span>xvf &lt;filename&gt;
</code></pre></div></div>
<p>where <em>&lt;filename&gt;</em> should start with <em>cudnn-x.y-</em> where <em>x.y</em> is the cuda version.</p>

<p>From the folder in which files were extracted, we copy these files like so:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo cp</span> &lt;extracted-folder-name&gt;/include/cudnn<span class="k">*</span>.h /usr/local/cuda/include 
<span class="nb">sudo cp</span> <span class="nt">-P</span> &lt;extracted-folder-name&gt;/lib/libcudnn<span class="k">*</span> /usr/local/cuda/lib64 
</code></pre></div></div>
<p>and change the permissions of these files to allow all users to read them:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo chmod </span>a+r /usr/local/cuda/include/cudnn<span class="k">*</span>.h /usr/local/cuda/lib64/libcudnn<span class="k">*</span>
</code></pre></div></div>
<p>Finally, we install tensorflow:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>tensorflow
</code></pre></div></div>
<p>This will also, likely, take time. After that, we’re done with the installation!</p>

<p>Now we can run this in a python shell to verify our</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="k">print</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">))</span>
</code></pre></div></div>

<p>If this shows a non-empty list, tensorflow recognizes the gpu and the necessary libraries.<sup id="fnref:numawarn" role="doc-noteref"><a href="#fn:numawarn" class="footnote" rel="footnote">3</a></sup> Typically, this output will look something like</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="n">PhysicalDevice</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'/physical_device:GPU:0'</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="s">'GPU'</span><span class="p">)]</span>
</code></pre></div></div>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:venv" role="doc-endnote">
      <p>This naturally calls for the use of a virtual environment specifically for development involving <code class="language-plaintext highlighter-rouge">tensorflow</code> but we will be skipping this adventure here. This is traditionally done with <strong>conda</strong> - as most of the guides recommend. However, <a href="https://stackoverflow.com/a/71058493/12983399">issues</a> have been reported on WSL using this method. We can use a typical virtual environment (<code class="language-plaintext highlighter-rouge">virtualenv</code>) instead. <a href="#fnref:venv" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:drivernote" role="doc-endnote">
      <p>The display driver must be installed on <strong>Windows</strong>. No separate installation of a graphics drivers is required on WSL. Standard cuda toolkits for Linux include the driver - so we cannot choose those versions. <a href="#fnref:drivernote" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:numawarn" role="doc-endnote">
      <p>If a warning is given by tensorflow about <code class="language-plaintext highlighter-rouge">Your kernel may have been built without NUMA support.</code>: this is probably nothing to worry about, as discussed in <a href="https://forums.developer.nvidia.com/t/numa-error-running-tensorflow-on-jetson-tx2/56119/4">this thread</a>. <a href="#fnref:numawarn" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/workflow/2022/07/31/tensorflow-cuda-wsl.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">ProgBlog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">ProgBlog</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/amzon-ex"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">amzon-ex</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog documenting progress</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
